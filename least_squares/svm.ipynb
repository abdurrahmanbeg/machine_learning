{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you need the predict the price of the house (i.e., column 4 of the csv file) and the features provided to you are 'len', 'width', 'rooms' (i.e., the first 3 columns of the csv file). You can use the sklearn library to use ``LinearRegression``, ``Ridge``, and ``Lasso`` from ``sklearn.linear_model``. Moreover, if you feel the need to expand the features to polynomials (say degree 2) you can either transform the CSV file manually or use the ``PolynomialFeatures`` from ``sklearn.preprocessing``. You might realize that adding polynomial features can improve the results but you have to be careful about overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Routines for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# Set label size for plots\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('LandPriceTrain.csv', delimiter=',')\n",
    "features = ['len', 'width', 'rooms']\n",
    "x_train = data[:,0:3] # predictors\n",
    "y_train = data[:,3] # response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('LandPriceTest.csv', delimiter=',')\n",
    "x_test = data[:,0:3] # predictors\n",
    "y_test = data[:,3] # response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What best can we acheive if we have no predictors and only response (House Prices) values in the training data? What will be the mean error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  84779.45\n",
      "Mean squared error:  3210718511.6474996\n",
      "Mean error:  44200.84\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y_pred = np.mean(y_train)\n",
    "print (\"Prediction: \", y_pred)\n",
    "print (\"Mean squared error: \", np.var(y_train))\n",
    "print (\"Mean error: \", np.mean(np.abs(y_pred-y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let's now use the features and see what we can observe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_subset_regression(x,y,flist):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 2):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of LinearRegression\n",
    "    regr = LinearRegression(fit_intercept=True, normalize=True, copy_X=True)\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 3010.83212779  2914.47821951 -2420.72225879]\n",
      "b =  -77044.07528278609\n",
      "Mean squared error (train):  217514383.15439206\n",
      "Mean error (train):  12629.08643427138\n",
      "Mean squared error (test):  158706743.78641912\n",
      "Mean error (test):  9999.972138004105\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2]\n",
    "regr = feature_subset_regression(x_train,y_train,flist)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. It seems we are underfitting as the train and test error are significantly high. Let's try to use polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try incorporating polynomial features (say of degree 2) and see how you perform on the train and the test set. You can either transform the CSV file manually or use the ``PolynomialFeatures`` from ``sklearn.preprocessing``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "#try to expand the fetaures fit the linear regression and report the results\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.fit_transform(x_test)\n",
    "print(np.shape(x_train_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [   0.         3087.2179578  2942.24531245]\n",
      "b =  -88410.28873001739\n",
      "Mean squared error (train):  224311218.08118543\n",
      "Mean error (train):  12908.28793341165\n",
      "Mean squared error (test):  8446371439.880243\n",
      "Mean error (test):  82436.86160080953\n"
     ]
    }
   ],
   "source": [
    "### UPDATE THE CODE BELOW ###\n",
    "flist = [0,1,2]\n",
    "regr = feature_subset_regression(x_train_poly,y_train,flist)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train_poly[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train_poly[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It seems we are overfitting as the train error is significantly lower than the test error. Let's try some regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_ridge(x,y,flist, alp):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 8):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of Ridge, be careful of the parameters\n",
    "    regr = Ridge(alp, fit_intercept=True, normalize=True, copy_X=True)\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [   0.          671.22857011  873.0746094  2763.47289999    7.59290655\n",
      "   55.65823143   13.74225506   18.5524101  -161.24098778]\n",
      "b =  -27692.892418302057\n",
      "Mean squared error (train):  46947651.7286587\n",
      "Mean error (train):  5668.600791740504\n",
      "Mean squared error (test):  69716998.73251502\n",
      "Mean error (test):  6606.276700825309\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "regr = feature_subset_ridge(x_train_poly,y_train,flist, 0.05)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train_poly[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train_poly[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test_poly[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test_poly[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_lasso(x,y,flist, alp):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 8):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of Lasso, be careful of the parameters\n",
    "    regr = Lasso(alp, fit_intercept=True, normalize=True, copy_X=True)\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 0.          0.          0.         -0.          0.         91.30207043\n",
      "  0.          1.1332545   0.        ]\n",
      "b =  5854.543815015029\n",
      "Mean squared error (train):  65329533.3564923\n",
      "Mean error (train):  6799.294735785111\n",
      "Mean squared error (test):  55470019.35749112\n",
      "Mean error (test):  5565.061009110918\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "regr = feature_subset_lasso(x_train_poly,y_train,flist, 1150)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train_poly[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train_poly[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test_poly[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test_poly[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document your observation and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add you observations and understanding:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially using the normal Linear Regression, we found high training and test error. This means we have to increase the complexity by increasing the features via polynomial.\n",
    "\n",
    "We found that the test error was significantly higher than the training error after the polynomial degree of 2 transformation.\n",
    "Due to this desparity, it seems clear that the model is \"memorizing\" the training data due to its high features. Regularization is a method of dealing with overfitting by controlling the complexity of the learning function. This is done via added penalty to the cost function if the weights of the features are high.\n",
    "\n",
    "After regularization, we still overfit slightly as the test error is still higher than the training error, but less signficantly.\n",
    "\n",
    "Now we perform feature selection using Lasso regression to remove unnecessary features, hoping to reduce the complexity.\n",
    "This gave us the desired objective of:\n",
    "    a) reduced error compared to linear regression\n",
    "    b) lower training error versus test error. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Questions (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement the losed-form solution for Ridge\n",
    "2. Implement the iterative solution (Gradient Descent) for Ridge\n",
    "3. Implement the iterative solution for Lasso\n",
    "4. Use the sklearn linear_model.ElasticNet and try on the above problem.\n",
    "\n",
    "Compare your implemented solutions with the built-in solutions on the above problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
